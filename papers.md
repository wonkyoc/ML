**LLMs**
* [SOSP'23] Efficient Memory Management for Large Language Model Serving with PagedAttention
* [OSDI'22] Orca: A Distributed Serving System for Transformer-Based Generative Models
* [ICML'23] FlexGen: High-throughput Generative Inference of Large Language Models with a Single GPU
* [arxiv] Fast Distributed Inference Serving for Large Language Models
* [NeurIPS'22] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
* [SC'23] DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale
* [ACL'23] PETALS: Collaborative Inference and Fine-tuning of Large Models

**Model Parallelism**
* [OSDI'23] AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving

**Parameter Efficient Fine-Tuning**
* [arxiv] LoRA: Low-Rank Adaptation of Large Language Models
* [arxiv] QLoRA: Efficient Finetuning of Quantized LLMs
* [ATC'22] PetS: A Unified Framework for Parameter-Efficient Transformers Serving

**Information Retrieval**
* [ACL'22] ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction
* [ACL'20] Dense Passage Retrieval for Open-Domain Question Answering
* [ACL'17] Reading Wikipedia to Answer Open-Domain Questions

**Retrieved Augmented Generation**
* [ICML'23] Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute
* [NeurIPS'20] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
* [arxiv] Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP

**Quantization**
* [ISCA'23] OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization



